# Sliced_ReLU_Attention
An Transformer implementation with sliced ReLU attention, sliced ReLU bump attention, and softmax attention
