# Sliced_ReLU_Attention
A Transformer implementation with sliced ReLU attention, sliced ReLU bump attention, and softmax attention.
