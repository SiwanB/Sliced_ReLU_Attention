# Sliced_ReLU_Attention

This repertory contains a PyTorch implementation of Sliced ReLU attention mechanisms.

## Paper:

You will find our paper here: https://arxiv.org/abs/2512.11411

